{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dengxing/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "with h5py.File('./Assignment_Data_set/train_128.h5', 'r') as H:\n",
    "    X= np.copy(H['data'])\n",
    "    X_train = X[0: 48000]\n",
    "    X_val = X[48000: 58000]\n",
    "    X_test_0 = X[58000:]\n",
    "with h5py.File('./Assignment_Data_set/train_label.h5', 'r') as H:\n",
    "    y = np.copy(H['label'])\n",
    "    y_train = y[0: 48000]\n",
    "    y_val = y[48000: 58000]\n",
    "    y_test_0 = y[58000:]\n",
    "with h5py.File('./Assignment_Data_set/test_128.h5', 'r') as H:\n",
    "    X_test = np.copy(H['data'])\n",
    "\n",
    "data = {}\n",
    "data['X_train'] = X_train\n",
    "data['X_val'] = X_val\n",
    "data['X_test_0'] = X_test_0\n",
    "data['y_train'] = y_train\n",
    "data['y_val'] = y_val\n",
    "data['y_test_0'] = y_test_0\n",
    "data['X_test'] = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the activation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_loss(X, y):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            X := softmax output\n",
    "            y := desired output\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "        \n",
    "        # The number of training data\n",
    "        N = X.shape[0]\n",
    "        loss = -np.sum(y * np.log(probs[np.arange(N), y])) / N\n",
    "        \n",
    "        # delta\n",
    "        dx = probs.copy()\n",
    "        dx[np.arange(N), y] -= 1\n",
    "        \n",
    "        # average the delta\n",
    "        dx /= N\n",
    "        return loss, dx\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x, w, b):\n",
    "        \"\"\"\n",
    "        Compute the forward pass for a fully-connected layer.\n",
    "        Inputs:\n",
    "            x := A np array containing input data of shape(N, n_in)\n",
    "            w := A np array of weights, of shape (n_in, n_out)\n",
    "            b := A np array of biases, of shape (M,)\n",
    "            \n",
    "        Returns:\n",
    "            out := output of shape (N, n_out)\n",
    "            cache := (x, w, b)\n",
    "        \"\"\"\n",
    "        \n",
    "        out = None\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # if the output is not complete, padding 0\n",
    "        x_rsp = x.reshape(N, -1)\n",
    "        \n",
    "        # output\n",
    "        out = x_rsp.dot(w) + b\n",
    "        cache = (x, w, b)\n",
    "        return out, cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Compute the backward pass for a fully-connected layer.\n",
    "        Inputs:\n",
    "            dout := the derivative of the output\n",
    "            cache := Tuple of\n",
    "                (x := input\n",
    "                 w := weights)\n",
    "        Returns a tutple of:\n",
    "            dx := gradient w.r.t x\n",
    "            dw := gradient w.r.t w\n",
    "            db := gradient w.r.t b\n",
    "        \"\"\"\n",
    "        \n",
    "        x, w, b = cache\n",
    "        dx, dw, db = None, None, None\n",
    "        \n",
    "        # The number of training data\n",
    "        N = x.shape[0]\n",
    "        x_rsp = x.reshape(N, -1)\n",
    "        \n",
    "        dx = dout.dot(w.T).reshape(*x.shape)\n",
    "        dw = x_rsp.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dx, dw, db\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_forward(x):\n",
    "        out = None\n",
    "        out = x * (x >= 0)\n",
    "        cache = x\n",
    "        return out, cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            dout := the derivative of the output\n",
    "            cache := input x, of same shape as dout\n",
    "        Returns:\n",
    "            dx := gradient w.r.t x\n",
    "        \"\"\"\n",
    "        dx, x = None, cache\n",
    "        dx = (x>=0) * dout\n",
    "        return dx\n",
    "    \n",
    "    @staticmethod\n",
    "    def bn_forward(x, gamma, beta, bn_param):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x := data of shape (N, D)\n",
    "            gamma := scale param of shape (D,)\n",
    "            beta := shift param of shape (D,)\n",
    "            bn_param := dict with the following keys:\n",
    "                mode := 'train' or 'test'; required\n",
    "                eps := Constant for numeric stability\n",
    "                momentum := Constant for running mean/variance\n",
    "                running_mean: Array of shape (D,) giving running mean of features\n",
    "            running_var Array of shape (D,) giving running variance of features\n",
    "        Returns a tuple of:\n",
    "            out: of shape (N, D)\n",
    "            cache: A tuple of values needed in the backward pass\n",
    "        \"\"\"\n",
    "        mode = bn_param['mode']\n",
    "        eps = bn_param.get('eps', 1e-5)\n",
    "        momentum = bn_param.get('momentum', 0.9)\n",
    "        \n",
    "        N, D = x.shape\n",
    "        running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "        running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "        \n",
    "        out, cache = None, None\n",
    "        if mode == 'train':\n",
    "            sample_mean = np.mean(x, axis=0)\n",
    "            sample_var = np.var(x, axis=0)\n",
    "            \n",
    "            x_hat = (x-sample_mean) / (np.sqrt(sample_var + eps))\n",
    "            out = gamma * x_hat + beta\n",
    "            \n",
    "            cache = (gamma, x, sample_mean, sample_var, eps, x_hat)\n",
    "            \n",
    "            \"\"\"\n",
    "            during testing, we need to recover the property\n",
    "            so we have to store the running mean and var.\n",
    "            Using momentum is because we want to add some randomlization\n",
    "            to avoid overfitting.\n",
    "            \"\"\"\n",
    "            running_mean = momentum * running_mean + (1-momentum) * sample_mean\n",
    "            running_var = momentum * running_var + (1-momentum) * sample_var\n",
    "        elif mode == 'test':\n",
    "            scale = gamma / (np.sqrt(running_var + eps))\n",
    "            out = x * scale + (beta - running_mean * scale)\n",
    "            \n",
    "        bn_param['running_mean'] = running_mean\n",
    "        bn_param['running_var'] = running_var\n",
    "        \n",
    "        return out, cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def bn_backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dout := derivatives of upstream\n",
    "            cache := variables of intermediates from bn_forward\n",
    "            \n",
    "        Returns a tuple of:\n",
    "            dx := gradient w.r.t input x\n",
    "            dgamma := gradient w.r.t scale param gamme\n",
    "            dbeta := gradient w.r.t shift parameter beta.\n",
    "        \"\"\"\n",
    "        dx, dgamma, dbeta = None, None, None\n",
    "        gamma, x, sample_mean, sample_var, eps, x_hat = cache\n",
    "        \n",
    "        # The numebr of training data\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # derivatives\n",
    "        dx_hat = dout*gamma\n",
    "        dvar = np.sum(dx_hat * (x-sample_mean) * -0.5 * np.power(sample_var + eps, -1.5), axis = 0)\n",
    "        dmean = np.sum(dx_hat * -1.0 / np.sqrt(sample_var + eps), axis = 0) + dvar * np.mean(-1 * (x-sample_mean), axis = 0)\n",
    "        dx = 1/np.sqrt(sample_var + eps) * dx_hat + dvar * 2.0 / N * (x-sample_mean) + 1.0 / N * dmean\n",
    "        dgamma = np.sum(x_hat * dout, axis = 0)\n",
    "        dbeta = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx, dgamma, dbeta\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_forward(x, dropout_param):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x := input data (activated), of any shape\n",
    "            dropout_param := a dict with the following keys:\n",
    "                p := dropout parameter. We drop each neuron output with probability p.\n",
    "                mode := 'test' or 'train'.\n",
    "                seed := Seed for the random number generator.\n",
    "            Outputs:\n",
    "                out := array of the same shape as x.\n",
    "                cache := A tuple (dropout_param, mask)\n",
    "        \"\"\"\n",
    "        p, mode = dropout_param['p'], dropout_param['mode']\n",
    "        if 'seed' in dropout_param:\n",
    "            np.random.seed(dropout_param['seed'])\n",
    "        \n",
    "        mask = None\n",
    "        out = None\n",
    "        \n",
    "        if mode == 'train':\n",
    "            \n",
    "            # Inverted dropout\n",
    "            mask = (np.random.rand(*x.shape) >= p) / (1-p)\n",
    "            out = x * mask\n",
    "        \n",
    "        elif mode == 'test':\n",
    "            out = x\n",
    "        \n",
    "        cache = (dropout_param, mask)\n",
    "        out = out.astype(x.dtype, copy = False)\n",
    "        \n",
    "        return out, cache\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dout := upstream derivatives, of any shape\n",
    "            cache := (dropout_param, mask) from dropout_forward.\n",
    "        \"\"\"\n",
    "        dropout_param, mask = cache\n",
    "        mode = dropout_param['mode']\n",
    "        dx = None\n",
    "        \n",
    "        if mode == 'train':\n",
    "            dx = dout * mask\n",
    "        elif mode == 'test':\n",
    "            dx = dout\n",
    "            \n",
    "        return dx\n",
    "    \n",
    "    \"\"\"\n",
    "    ======\n",
    "    The following code assembles the function above.\n",
    "    General structure:\n",
    "    Net values -> activation layer\n",
    "    ======\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def nn_relu_forward(x, w, b):\n",
    "        \"\"\"\n",
    "        Layer followed by a ReLU\n",
    "        \n",
    "        Inputs:\n",
    "            x := Input to the layer\n",
    "            w, b := Weights for the layer\n",
    "        \n",
    "        Returns:\n",
    "            out := output from the ReLU\n",
    "            cache := Object to give to the backward pass\n",
    "        \"\"\"\n",
    "        a, fc_cache = Activation.forward(x,w,b)\n",
    "        out, relu_cache = Activation.relu_forward(a)\n",
    "        cache = (fc_cache, relu_cache)\n",
    "        \n",
    "        return out, cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def nn_relu_backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the nn layer with ReLU\n",
    "        \"\"\"\n",
    "        fc_cache, relu_cache = cache\n",
    "        \n",
    "        # dout / da, where a = net\n",
    "        # da := derivative w.r.t the ReLU layer\n",
    "        da = Activation.relu_backward(dout, relu_cache)\n",
    "        \n",
    "        # derivatives w.r.t the weight, x, b\n",
    "        dx, dw, db = Activation.backward(da, fc_cache)\n",
    "        \n",
    "        return dx, dw, db\n",
    "    \n",
    "    @staticmethod\n",
    "    def nn_bn_relu_forward(x, w, b, gamma, beta, bn_param):\n",
    "        \"\"\"\n",
    "        The batch normalized version of Relu, here is the structure\n",
    "        BN -> Net values -> ReLU -> output\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        a := the net values of WX\n",
    "        fc_cache := a cache of the x, w, and b for further process.\n",
    "        \"\"\"\n",
    "        a, fc_cache = Activation.forward(x, w, b)\n",
    "        bn, bn_cache = Activation.bn_forward(a, gamma, beta, bn_param)\n",
    "        out, relu_cache = Activation.relu_forward(bn)\n",
    "        cache = (fc_cache, bn_cache, relu_cache)\n",
    "        \n",
    "        return out, cache\n",
    "    \n",
    "    @staticmethod\n",
    "    def nn_bn_relu_backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the bn nn layer with ReLU\n",
    "        Returns:\n",
    "            dx := derivative w.r.t input x\n",
    "            dw := derivative w.r.t weight\n",
    "            db := derivative w.r.t biases\n",
    "            dgamma := derivative w.r.t bn param gamma\n",
    "            dbeta := derivative w.r.t bn param beta\n",
    "        \"\"\"\n",
    "        fc_cache, bn_cache, relu_cache = cache\n",
    "        \n",
    "        # dout / dbn\n",
    "        dbn = Activation.relu_backward(dout, relu_cache)\n",
    "        da, dgamma, dbeta = Activation.bn_backward(dbn, bn_cache)\n",
    "        dx, dw, db = Activation.backward(da, fc_cache)\n",
    "        \n",
    "        return dx, dw, db, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Network\n",
    "Multi-layer neural network with the following architecture:  \n",
    "{Net - {BN} - ReLU - {dropout}} x (L-1) - Net - SoftMax_with_weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, hidden_dims, \n",
    "               input_dim = 128, \n",
    "               num_classes = 10,\n",
    "               dropout = 0, \n",
    "               use_bn = True,\n",
    "               reg = 0.0,\n",
    "               dtype = np.float32,\n",
    "               seed = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            hidden_dims := an array, the i-th value indicates the # of neurals on the i-th layer\n",
    "            input_dim := the # of attributes an input has\n",
    "            num_classes := target classes\n",
    "            dropout := the probability of dropout\n",
    "            use_bn := the model uses batch_normalization or not\n",
    "        \"\"\"\n",
    "        self.use_bn = use_bn\n",
    "        self.use_dropout = dropout > 0\n",
    "        self.reg =reg\n",
    "        self.dtype = dtype\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.params = {}\n",
    "        layer_input_dim = input_dim\n",
    "        \n",
    "        \"\"\"\n",
    "        ======\n",
    "        Initializing network\n",
    "        ======\n",
    "        \"\"\"\n",
    "        # Initialize the hidden layers\n",
    "        for i, hd in enumerate(hidden_dims):\n",
    "            # Generate weights using Xavier\n",
    "            self.params['W%d' % (i+1)] = np.random.uniform(\n",
    "                low = -np.sqrt(6. / (layer_input_dim + hd)),\n",
    "                high = np.sqrt(6. / (layer_input_dim + hd)),\n",
    "                size = (layer_input_dim, hd)\n",
    "            )\n",
    "            self.params['b%d' % (i+1)] = np.zeros(hd)\n",
    "            \n",
    "            # If use batch normalization, we need to set up gamma and beta\n",
    "            # They are originally set as 1s and 0s\n",
    "            if self.use_bn:\n",
    "                self.params['gamma%d' % (i+1)] = np.ones(hd)\n",
    "                self.params['beta%d' % (i+1)] = np.zeros(hd)\n",
    "            \n",
    "            # Move to next layer\n",
    "            layer_input_dim = hd\n",
    "            \n",
    "        # Initialize the output Layers\n",
    "        self.params['W%d' % (self.num_layers)] = np.random.uniform(\n",
    "                low = -np.sqrt(6. / (layer_input_dim + hd)),\n",
    "                high = np.sqrt(6. / (layer_input_dim + hd)),\n",
    "                size = (layer_input_dim, num_classes)\n",
    "            )\n",
    "        self.params['b%d' % (self.num_layers)] = np.zeros(num_classes)\n",
    "        \n",
    "        # In cases, casting all parameters to the correct data type\n",
    "        for k,v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "        \n",
    "        \"\"\"\n",
    "        ======\n",
    "        If use dropout\n",
    "        ======\n",
    "        \"\"\"\n",
    "        self.dropout_param = {}\n",
    "        if self.use_dropout:\n",
    "            self.dropout_param = {'mode' : 'train', 'p' : dropout}\n",
    "            if seed is not None:\n",
    "                self.dropout_param['seed'] = seed\n",
    "                \n",
    "        \"\"\"\n",
    "        ======\n",
    "        If use batch normalization\n",
    "        ======\n",
    "        \"\"\"\n",
    "        self.bn_params = []\n",
    "        if self.use_bn:\n",
    "            self.bn_params = [{'mode' : 'train'} for i in range(self.num_layers-1)]\n",
    "            \n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for the nn.\n",
    "        Inputs:\n",
    "            X := array of input data with shape (N, d_1, ..., d_k)\n",
    "            y := array of labels with shape (N,). y[i] gives the label for the i-th input\n",
    "        \n",
    "        Returns:\n",
    "            loss := cross-entropy loss\n",
    "            gradient := gradient\n",
    "        \"\"\"\n",
    "        \n",
    "        # Cast input\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "        \n",
    "        \n",
    "        # change the dropout & bn mode\n",
    "        if self.use_dropout:\n",
    "            self.dropout_param['mode'] = mode\n",
    "        if self.use_bn:\n",
    "            for bn_param in self.bn_params:\n",
    "                bn_param['mode'] = mode\n",
    "            \n",
    "        scores = None\n",
    "        \n",
    "        # forward pass for the network\n",
    "        layer_input = X\n",
    "        ar_cache = {}\n",
    "        dp_cache = {}\n",
    "        \n",
    "        for lay in range(self.num_layers-1):\n",
    "            # BN the input\n",
    "            if self.use_bn:\n",
    "                # use the nn_bn_relu_forward/backward\n",
    "                layer_input, ar_cache[lay] = Activation.nn_bn_relu_forward(\n",
    "                    layer_input,\n",
    "                    self.params['W%d' %(lay+1)],\n",
    "                    self.params['b%d' %(lay+1)],\n",
    "                    self.params['gamma%d' %(lay+1)],\n",
    "                    self.params['beta%d' %(lay+1)],\n",
    "                    self.bn_params[lay]\n",
    "                )\n",
    "            else:\n",
    "                layer_input, ar_cache[lay] = Activation.relu_forward(lay, self.params['W%d' %(lay+1), self.params['b%d' %(lay+1)]])\n",
    "        \n",
    "            if self.use_dropout:\n",
    "                layer_input, dp_cache[lay] = Activation.dropout_forward(layer_input, self.dropout_param)\n",
    "\n",
    "        ar_out, ar_cache[self.num_layers] = Activation.forward(layer_input, self.params['W%d'%(self.num_layers)], self.params['b%d'%(self.num_layers)])\n",
    "        scores = ar_out\n",
    "        \n",
    "        \"\"\"\n",
    "        Return the raw prediction\n",
    "        The model is not responsible for outputing the exactly correct output but the row values\n",
    "        i.e.:\n",
    "            [0.8327949832,12312312,4543,0.5454]\n",
    "        instead of\n",
    "            [0, 1, 0, 0] (i.e. the prediction says the input belongs to class 2)\n",
    "        \n",
    "        The solver takes the job\n",
    "        \"\"\"\n",
    "        if mode == 'test':\n",
    "            return scores\n",
    "        \n",
    "        loss, grads = 0.0, {}\n",
    "        \n",
    "        \"\"\"\n",
    "        ======\n",
    "        The following code block calculates the softmax loss and gradients\n",
    "        using weight decay, it add L2 regularization to loss function\n",
    "        ======\n",
    "        \"\"\"\n",
    "        loss, dscores = Activation.softmax_loss(scores, y)\n",
    "        dhout = dscores\n",
    "        \n",
    "        # Weight decay\n",
    "        loss = loss + 0.5 * self.reg * np.sum(self.params['W%d'%(self.num_layers)] * self.params['W%d'%(self.num_layers)])\n",
    "        dx, dw, db = Activation.backward(dhout, ar_cache[self.num_layers])\n",
    "        \n",
    "        # Gradient\n",
    "        grads['W%d' %(self.num_layers)] = dw + self.reg * self.params['W%d' % (self.num_layers)]\n",
    "        grads['b%d' % (self.num_layers)] = db\n",
    "        \n",
    "        # Update gradient\n",
    "        dhout = dx\n",
    "        \n",
    "        # Backpropagation\n",
    "        for idx in range(self.num_layers-1):\n",
    "            lay = self.num_layers - 1 - idx - 1\n",
    "            loss = loss + 0.5 * self.reg * np.sum(self.params['W%d'%(lay+1)] * self.params['W%d'%(lay+1)])\n",
    "            \n",
    "            if self.use_dropout:\n",
    "                dhout = Activation.dropout_backward(dhout, dp_cache[lay])\n",
    "                \n",
    "            if self.use_bn:\n",
    "                dx, dw, db, dgamma, dbeta = Activation.nn_bn_relu_backward(dhout, ar_cache[lay])\n",
    "            else:\n",
    "                dx, dw, db = Activation.relu_backward(dhout, ar_cache[lay])\n",
    "                \n",
    "            grads['W%d' % (lay+1)] = dw + self.reg * self.params['W%d' %(lay+1)]\n",
    "            grads['b%d' % (lay+1)] = db\n",
    "            \n",
    "            if self.use_bn:\n",
    "                grads['gamma%d' % (lay+1)] = dgamma\n",
    "                grads['beta%d' % (lay+1)] = dbeta\n",
    "            dhout = dx\n",
    "        return loss, grads\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer: SGD with Momentum\n",
    "     Stochastic gradient descent with more configures.\n",
    "     - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "     - velocity: A numpy array of the same shape as w and dw used to store a moving average of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optim:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent without momentum\n",
    "    Inputs:\n",
    "        w := original weights\n",
    "        dw := gradients\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def sgd(w, dw, config = None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        config.setdefault('learning_rate', 1e-2)\n",
    "        w -= config['learning_rate'] * dw\n",
    "        return w, config\n",
    "        \n",
    "    @staticmethod\n",
    "    def sgd_momentum(w, dw, config = None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        config.setdefault('learning_rate', 1e-2)\n",
    "        config.setdefault('momentum', 0.9)\n",
    "        \n",
    "        \"\"\"\n",
    "        Fetching previous velocity\n",
    "        if none, set to zeros\n",
    "        \"\"\"\n",
    "        v = config.get('velocity', np.zeros_like(w))\n",
    "        next_w = None\n",
    "        \n",
    "        \"\"\"\n",
    "        Computing new velocity\n",
    "        \"\"\"\n",
    "        v = config['momentum'] * v - config['learning_rate'] * dw\n",
    "        next_w = w + v\n",
    "        config['velocity'] = v\n",
    "        \n",
    "        return next_w, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver\n",
    "    A Solver encapsulates all the logic necessary for training classification  models. \n",
    "    The Solver performs stochastic gradient descent using different update rules defined in optim.py.\n",
    "    The solver accepts both training and validataion data and labels so it can periodically check classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model := the multi-layer nn model\n",
    "        data := a dict, with the following keys\n",
    "            X_train := training set\n",
    "            y_train := labels for the training set\n",
    "            X_val := validation set\n",
    "            y_val = validation set's labels\n",
    "    \"\"\"\n",
    "    def __init__(self, model, data, **kwargs):\n",
    "        self.model = model\n",
    "        self.X_train = data['X_train']\n",
    "        self.y_train = data['y_train']\n",
    "        self.X_val = data['X_val']\n",
    "        self.y_val = data['y_val']\n",
    "        \n",
    "        # Unpack keyward args\n",
    "        self.update_rule = kwargs.pop('update_rule', 'sgd')\n",
    "        self.optim_config = kwargs.pop('optim_config', {})\n",
    "        self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
    "        self.batch_size = kwargs.pop('batch_size', 100)\n",
    "        self.num_epochs = kwargs.pop('num_epochs', 10)\n",
    "        \n",
    "        # Print the result (loss, accuracy) every n epochs\n",
    "        self.print_every = kwargs.pop('print_every', 10)\n",
    "        self.verbose = kwargs.pop('verbose', True)\n",
    "\n",
    "        # Check if the update rule exists or not\n",
    "        if not hasattr(Optim, self.update_rule):\n",
    "            raise ValueError('Invalid update rule \"%s\"' % self.update_rule)\n",
    "        \n",
    "        self.update_rule = getattr(Optim, self.update_rule)\n",
    "        \n",
    "        self._reset()\n",
    "        \n",
    "    \"\"\"\n",
    "    ======\n",
    "    Reset primary parameters for further training\n",
    "    Parameters:\n",
    "        epoch := the number of current epoch\n",
    "        best_val_acc := the best accuracy of our model for validation set\n",
    "        best_params := the best weights, biases, gamma and beta(if has)\n",
    "        loss_history := for printing training history\n",
    "    ======\n",
    "    \"\"\"\n",
    "    def _reset(self):\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "        \n",
    "        # Make a deep copy of the optim_config for each parameter\n",
    "        self.optim_configs = {}\n",
    "        for p in self.model.params:\n",
    "            d = {k : v for k,v in self.optim_config.items()}\n",
    "            self.optim_configs[p] = d\n",
    "            \n",
    "    \"\"\"\n",
    "    ======\n",
    "    Update the gradient of every parameters\n",
    "    Utilize minibatch\n",
    "    ======\n",
    "    \"\"\"\n",
    "    def _step(self):\n",
    "        \n",
    "        # Making\n",
    "        num_train = self.X_train.shape[0]\n",
    "        # Randomly choose the training data\n",
    "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
    "        X_batch = self.X_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "        \n",
    "        # Compute loss and grad\n",
    "        loss, grads = self.model.loss(X_batch, y_batch)\n",
    "        self.loss_history.append(loss)\n",
    "        \n",
    "        # Updat nn's parameter\n",
    "        for p, w in self.model.params.items():\n",
    "            # p := the keyof parameters in the nn, could be weights, biases, gamma or beta\n",
    "            # w is similar, can not only be weight but biases, gamma or beta, velocity etc.\n",
    "            dw = grads[p]\n",
    "            config = self.optim_configs[p]\n",
    "            next_w, next_config = self.update_rule(w, dw, config)\n",
    "            self.model.params[p] = next_w\n",
    "            self.optim_configs[p] = next_config\n",
    "        \n",
    "    def check_accuracy(self, X, y, num_samples = None, batch_size = 100):\n",
    "        # The number of training data\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        if num_samples is not None and N > num_samples:\n",
    "            mask = np.random.choice(N, num_samples)\n",
    "            N = num_samples\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "        \n",
    "        # Compute prediction in batches\n",
    "        num_batches = N // batch_size\n",
    "        if N % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i+1) * batch_size\n",
    "            scores = self.model.loss(X[start:end])\n",
    "            # get the prediction\n",
    "            y_pred.append(np.argmax(scores, axis=1))\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        acc = np.mean(y_pred == y)\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    \"\"\"\n",
    "    ======\n",
    "    Train the model\n",
    "    ======\n",
    "    \"\"\"\n",
    "    def train(self):\n",
    "        num_train = self.X_train.shape[0]\n",
    "        \n",
    "        # How many batches are run in each epoch\n",
    "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
    "        \n",
    "        # Total number of batches\n",
    "        num_iterations = self.num_epochs * iterations_per_epoch\n",
    "        \n",
    "        for t in range(num_iterations):\n",
    "            # Tran the nn using only one batch\n",
    "            self._step()\n",
    "            \n",
    "            # Print the result after one epoch\n",
    "            if t % self.print_every == 0:\n",
    "                print('(Iteration %d / %d) loss: %f' % (t+1, num_iterations, self.loss_history[-1]))\n",
    "                \n",
    "            # At the end of every epoch, increment the epoch counter and decay the learning rate\n",
    "            epoch_end = (t+1) % iterations_per_epoch == 0\n",
    "            if epoch_end:\n",
    "                self.epoch += 1\n",
    "                for k in self.optim_configs:\n",
    "                    self.optim_configs[k]['learning_rate'] *= self.lr_decay\n",
    "                    \n",
    "            first_it = (t == 0)\n",
    "            last_it = (t == num_iterations + 1)\n",
    "            if first_it or last_it or epoch_end:\n",
    "                train_acc = self.check_accuracy(self.X_train, self.y_train, num_samples = 1000)\n",
    "                val_acc = self.check_accuracy(self.X_val, self.y_val)\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                self.val_acc_history.append(val_acc)\n",
    "                \n",
    "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (self.epoch, self.num_epochs, train_acc, val_acc))\n",
    "                \n",
    "                # Similar to the early stop, but keep training until the end\n",
    "                if val_acc > self.best_val_acc:\n",
    "                    self.best_val_acc = val_acc\n",
    "                    self.best_params = {}\n",
    "                    # Deep copy the params\n",
    "                    for k, v in self.model.params.items():\n",
    "                        self.best_params[k] = v.copy()\n",
    "            # At the end of training, only keep the best params\n",
    "            self.model.params = self.best_params\n",
    "            \n",
    "        \"\"\"\n",
    "        Predict the inputs X\n",
    "        Inputs:\n",
    "            X := a matrix of shape (N, d_1, ..., d_k), where N := the number of inputs, and k := the number of attributes\n",
    "        Returns:\n",
    "            y_pred := the prediction arrary of shape (N,), where the i-th entry is the prediction of the i-th input.\n",
    "        \"\"\"\n",
    "        def predict(self, X):\n",
    "            y_pred = []\n",
    "            \n",
    "            # the raw prediction\n",
    "            scores = self.model.loss(X)\n",
    "            \n",
    "            # appending the prediction\n",
    "            y_pred.append(np.argmax(scores, axis=1))\n",
    "            \n",
    "            # make the predictions be horizontal\n",
    "            y_pred = np.hstack(y_pred)\n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "    Train the best fully-connected model with the giving dataset, storing your best model in the best_model variable. And then compute the  validating and test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 24000) loss: 22.764764\n",
      "(Epoch 0 / 50) train acc: 0.100000; val_acc: 0.103900\n",
      "(Epoch 1 / 50) train acc: 0.825000; val_acc: 0.836100\n",
      "(Iteration 501 / 24000) loss: 5.546189\n",
      "(Epoch 2 / 50) train acc: 0.855000; val_acc: 0.856300\n",
      "(Iteration 1001 / 24000) loss: 3.722274\n",
      "(Epoch 3 / 50) train acc: 0.868000; val_acc: 0.857900\n",
      "(Iteration 1501 / 24000) loss: 2.097455\n",
      "(Epoch 4 / 50) train acc: 0.861000; val_acc: 0.868100\n",
      "(Iteration 2001 / 24000) loss: 1.652761\n",
      "(Epoch 5 / 50) train acc: 0.865000; val_acc: 0.846900\n",
      "(Iteration 2501 / 24000) loss: 1.823375\n",
      "(Epoch 6 / 50) train acc: 0.891000; val_acc: 0.867600\n",
      "(Iteration 3001 / 24000) loss: 1.749654\n",
      "(Epoch 7 / 50) train acc: 0.896000; val_acc: 0.870100\n",
      "(Iteration 3501 / 24000) loss: 1.576010\n",
      "(Epoch 8 / 50) train acc: 0.894000; val_acc: 0.863900\n",
      "(Iteration 4001 / 24000) loss: 1.906359\n",
      "(Epoch 9 / 50) train acc: 0.873000; val_acc: 0.865500\n",
      "(Iteration 4501 / 24000) loss: 1.824426\n",
      "(Epoch 10 / 50) train acc: 0.872000; val_acc: 0.869600\n",
      "(Iteration 5001 / 24000) loss: 1.919088\n",
      "(Epoch 11 / 50) train acc: 0.896000; val_acc: 0.879800\n",
      "(Iteration 5501 / 24000) loss: 1.570698\n",
      "(Epoch 12 / 50) train acc: 0.885000; val_acc: 0.875100\n",
      "(Iteration 6001 / 24000) loss: 1.623385\n",
      "(Epoch 13 / 50) train acc: 0.891000; val_acc: 0.881600\n",
      "(Iteration 6501 / 24000) loss: 1.991419\n",
      "(Epoch 14 / 50) train acc: 0.898000; val_acc: 0.879400\n",
      "(Iteration 7001 / 24000) loss: 1.958991\n",
      "(Epoch 15 / 50) train acc: 0.886000; val_acc: 0.882700\n",
      "(Iteration 7501 / 24000) loss: 1.414633\n",
      "(Epoch 16 / 50) train acc: 0.897000; val_acc: 0.883400\n",
      "(Iteration 8001 / 24000) loss: 1.371573\n",
      "(Epoch 17 / 50) train acc: 0.913000; val_acc: 0.888400\n",
      "(Iteration 8501 / 24000) loss: 1.033873\n",
      "(Epoch 18 / 50) train acc: 0.901000; val_acc: 0.882300\n",
      "(Iteration 9001 / 24000) loss: 0.988102\n",
      "(Epoch 19 / 50) train acc: 0.899000; val_acc: 0.884400\n",
      "(Iteration 9501 / 24000) loss: 2.450362\n",
      "(Epoch 20 / 50) train acc: 0.914000; val_acc: 0.883700\n",
      "(Iteration 10001 / 24000) loss: 1.539598\n",
      "(Epoch 21 / 50) train acc: 0.917000; val_acc: 0.888000\n",
      "(Iteration 10501 / 24000) loss: 1.370220\n",
      "(Epoch 22 / 50) train acc: 0.915000; val_acc: 0.890200\n",
      "(Iteration 11001 / 24000) loss: 1.332480\n",
      "(Epoch 23 / 50) train acc: 0.930000; val_acc: 0.894000\n",
      "(Iteration 11501 / 24000) loss: 1.113100\n",
      "(Epoch 24 / 50) train acc: 0.908000; val_acc: 0.889900\n",
      "(Epoch 25 / 50) train acc: 0.907000; val_acc: 0.892400\n",
      "(Iteration 12001 / 24000) loss: 1.294341\n",
      "(Epoch 26 / 50) train acc: 0.926000; val_acc: 0.894900\n",
      "(Iteration 12501 / 24000) loss: 1.566647\n",
      "(Epoch 27 / 50) train acc: 0.935000; val_acc: 0.897600\n",
      "(Iteration 13001 / 24000) loss: 0.698966\n",
      "(Epoch 28 / 50) train acc: 0.916000; val_acc: 0.896500\n",
      "(Iteration 13501 / 24000) loss: 0.930407\n",
      "(Epoch 29 / 50) train acc: 0.936000; val_acc: 0.895700\n",
      "(Iteration 14001 / 24000) loss: 1.391222\n",
      "(Epoch 30 / 50) train acc: 0.932000; val_acc: 0.898500\n",
      "(Iteration 14501 / 24000) loss: 0.792057\n",
      "(Epoch 31 / 50) train acc: 0.933000; val_acc: 0.898400\n",
      "(Iteration 15001 / 24000) loss: 0.566083\n",
      "(Epoch 32 / 50) train acc: 0.925000; val_acc: 0.897600\n",
      "(Iteration 15501 / 24000) loss: 0.864151\n",
      "(Epoch 33 / 50) train acc: 0.934000; val_acc: 0.901200\n",
      "(Iteration 16001 / 24000) loss: 1.050774\n",
      "(Epoch 34 / 50) train acc: 0.934000; val_acc: 0.904400\n",
      "(Iteration 16501 / 24000) loss: 1.064939\n",
      "(Epoch 35 / 50) train acc: 0.930000; val_acc: 0.900400\n",
      "(Iteration 17001 / 24000) loss: 0.643413\n",
      "(Epoch 36 / 50) train acc: 0.933000; val_acc: 0.903600\n",
      "(Iteration 17501 / 24000) loss: 0.567272\n",
      "(Epoch 37 / 50) train acc: 0.934000; val_acc: 0.901800\n",
      "(Iteration 18001 / 24000) loss: 1.568512\n",
      "(Epoch 38 / 50) train acc: 0.941000; val_acc: 0.902200\n",
      "(Iteration 18501 / 24000) loss: 1.529516\n",
      "(Epoch 39 / 50) train acc: 0.951000; val_acc: 0.903800\n",
      "(Iteration 19001 / 24000) loss: 0.549142\n",
      "(Epoch 40 / 50) train acc: 0.935000; val_acc: 0.902900\n",
      "(Iteration 19501 / 24000) loss: 0.809867\n",
      "(Epoch 41 / 50) train acc: 0.953000; val_acc: 0.903800\n",
      "(Iteration 20001 / 24000) loss: 1.127840\n",
      "(Epoch 42 / 50) train acc: 0.951000; val_acc: 0.906100\n",
      "(Iteration 20501 / 24000) loss: 0.854814\n",
      "(Epoch 43 / 50) train acc: 0.946000; val_acc: 0.903800\n",
      "(Iteration 21001 / 24000) loss: 0.552341\n",
      "(Epoch 44 / 50) train acc: 0.949000; val_acc: 0.905100\n",
      "(Iteration 21501 / 24000) loss: 0.531014\n",
      "(Epoch 45 / 50) train acc: 0.952000; val_acc: 0.905700\n",
      "(Iteration 22001 / 24000) loss: 0.752027\n",
      "(Epoch 46 / 50) train acc: 0.955000; val_acc: 0.904600\n",
      "(Iteration 22501 / 24000) loss: 0.272581\n",
      "(Epoch 47 / 50) train acc: 0.957000; val_acc: 0.905100\n",
      "(Iteration 23001 / 24000) loss: 1.412883\n",
      "(Epoch 48 / 50) train acc: 0.951000; val_acc: 0.907500\n",
      "(Iteration 23501 / 24000) loss: 0.848220\n",
      "(Epoch 49 / 50) train acc: 0.960000; val_acc: 0.905500\n",
      "(Epoch 50 / 50) train acc: 0.962000; val_acc: 0.906000\n",
      "test acc: 0.905500\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_model = None\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = Model([600, 500, 400, 300, 200, 100], dtype = np.float64, dropout = 0.25, use_bn = True, reg = 1e-2)\n",
    "solver = Solver(model, data, print_every=500, num_epochs = 50, batch_size = 100,\n",
    "               update_rule = 'sgd_momentum',\n",
    "               optim_config = {\n",
    "                   'learning_rate' : learning_rate,\n",
    "               },\n",
    "               lr_decay = 0.9)\n",
    "\n",
    "solver.train()\n",
    "scores = model.loss(data['X_test_0'])\n",
    "y_pred = np.argmax(scores, axis = 1)\n",
    "acc = np.mean(y_pred == data['y_test_0'])\n",
    "print ('test acc: %f' %(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the acc and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy:  0.906\n",
      "Test set accuracy:  0.9055\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3XecXFX9+P/Xe2Zne082PSGFUEIglBCa0jsKWBBR/KCg6OcHX8EOih+xI1bsIiKIhWqJigSkd5IACQkQUkjZtE2yvc1Oef/+OGc3s5vZ3dnd2Z0t7+fjMY+5c+s5c2fu+55zzz1XVBVjjDEmHQKZToAxxpjRw4KKMcaYtLGgYowxJm0sqBhjjEkbCyrGGGPSxoKKMcaYtLGgYkyaichHReSZHqb/R0QuG8o0GTNULKiYUUtENorI6ZlOR1eqeo6q3tnbfCKiIrL/UKTJmHSxoGLMKCQiWZlOgxmbLKiYMUlEPiEi60SkWkQWi8gUP15E5MciUiUidSKyUkTm+2nnisjrItIgIltF5PO9bOMHIlIjIm+LyDkJ458QkY/74f1F5Em/rd0ico8f/5SffYWINIrIxT2l209TEblKRNYCa0XkFyLywy5p+qeIXDvwb9CY5CyomDFHRE4Fvgt8AJgMbALu9pPPBE4EDgBKgYuBPX7a74BPqmoRMB94rIfNHAOsAcYDNwO/ExFJMt83gYeBMmAa8DMAVT3RT1+gqoWqek8v6W53od/2POBO4BIRCfh8jwdOA/7SQ7qNGRALKmYs+jBwu6q+rKph4HrgOBGZCUSAIuAgQFT1DVXd7peLAPNEpFhVa1T15R62sUlVf6uqMdzBfTIwMcl8EWA/YIqqtqpqtxf4e0l3u++qarWqtqjqS0AdLpAAfBB4QlV39rANYwbEgooZi6bgzvIBUNVGXGlkqqo+Bvwc+AWwU0RuFZFiP+v7gHOBTb7K6rgetrEjYf3NfrAwyXxfBAR4SURWi8jl/Ul3wjxbuixzJ3CpH74UuKuH9RszYBZUzFi0DVc6AEBECoBxwFYAVf2pqh4FHIKrBvuCH79UVS8AJgB/B+4daEJUdYeqfkJVpwCfBH7ZQ4uvHtPdvsouy/wRuEBEFgAH+3QbM2gsqJjRLiQiuQmvLODPwMdE5HARyQG+A7yoqhtF5GgROUZEQkAT0ArERCRbRD4sIiWqGgHqgdhAEyciF4nINP+xBhcU2te7E5idMHu36e5u/apaCSzFlVAeUNWWgabZmJ5YUDGj3YNAS8LrRlV9FPgq8ACwHZiDu94AUAz8FneA34SrXvqBn/YRYKOI1AOfYm+10kAcDbwoIo3AYuAaVX3bT7sRuFNEakXkA72kuyd3AodiVV9mCIg9pMuY0U1ETsRVg81U1Xim02NGNyupGDOK+Wq8a4DbLKCYoWBBxZhRSkQOBmpxzZl/kuHkmDHCqr+MMcakjZVUjDHGpM2o6XRu/PjxOnPmzEwnwxhjRpTly5fvVtWKdK1v1ASVmTNnsmzZskwnwxhjRhQR2dT7XKmz6i9jjDFpY0HFGGNM2oz5oBKOxjjx5sd5fE1VppNijDEj3pgPKpU1LWyubuYb/3w900kxxpgRb8wHlbxQEIBDphT3MqcxxpjejPmgkhV0D+N7Zt3uDKfEGGNGvjEfVGJx16NAbXMkwykxxpiRb8wHlbL8bAA+fWp3z0UyxhiTqjEfVMTVfpHjr60YY4zpPwsquKhiHWsaY8zAjfmgEvAlFYspxhgzcGM+qIiv/4pbUDHGmAHLeFARkeki8riIvCEiq0XkGj++XEQeEZG1/r1sMLbfUVLBoooxxgxUxoMKEAU+p6oHA8cCV4nIPOA64FFVnQs86j+nnZVUjDEmfTIeVFR1u6q+7IcbgDeAqcAFwJ1+tjuBCwczHU9Y31/GGDNgGQ8qiURkJnAE8CIwUVW3gws8wIQk818pIstEZNmuXbsGtO2VlXUDWt4YY8wwCioiUgg8AFyrqvWpLKOqt6rqQlVdWFGRtgeXGWOM6adhEVREJIQLKH9S1b/60TtFZLKfPhmw+iljjBnmMh5UxF0p/x3whqr+KGHSYuAyP3wZ8I+hTpsxxpi+GQ7PqD8B+Ajwmoi86sd9GbgJuFdErgA2AxdlKH3GGGNSlPGgoqrPgO8rZV+nDWVajDHGDEzGq7+MMcaMHhZUjDHGpI0FFWOMMWljQcUYY0zaWFAxxhiTNhZUjDHGpI0FFWOMMWljQcUYY0zaWFAxxhiTNhZUjDHGpI0FFWOMMWljQcUYY0zaWFAxxhiTNhZUjDHGpI0FlQSN4Wimk2CMMSOaBZUETRZUjDFmQCyoJIjE4plOgjHGjGgWVBKEoxZUjDFmICyoJKhviWQ6CcYYM6KlNaiIyBwRyfHDJ4vIp0WkNJ3bGEwPvFyZ6SQYY8yIlu6SygNATET2B34HzAL+nOZtDJo3tzdkOgnGGDOipTuoxFU1CrwH+ImqfgaYnOZtDJplm2oynQRjjBnR0h1UIiJyCXAZ8C8/LpTmbRhjjBmm0h1UPgYcB3xbVd8WkVnAH9O8DWOMMcNUVjpXpqqvA58GEJEyoEhVb0rnNowxxgxf6W799YSIFItIObAC+L2I/Cid2xgMnz51/0wnwRhjRoV0V3+VqGo98F7g96p6FHB6mreRdv/vtLkdw6qawZQYY8zIlu6gkiUik4EPsPdC/bAXCu79GnY1hjOYEmOMGdnSHVS+ASwB1qvqUhGZDaxN8zYG1aJvP5rpJBhjzIiV7gv19wH3JXzeALwvndswxhgzfKX7Qv00EfmbiFSJyE4ReUBEpvWyzO1+/lUJ48pF5BERWevfy9KZTmOMMYMj3dVfvwcWA1OAqcA//bie3AGc3WXcdcCjqjoXeNR/NsYYM8ylO6hUqOrvVTXqX3cAFT0toKpPAdVdRl8A3OmH7wQuTHM6jTHGDIJ0B5XdInKpiAT961JgTz/WM1FVtwP49wnJZhKRK0VkmYgs27Vr1wCSDYdP39uZ8rPrdg9oXcYYM1alO6hcjmtOvAPYDrwf13XLoFDVW1V1oaourKjosUDUq/GFOR3DH77txYEmzRhjxqS0BhVV3ayq56tqhapOUNULcTdC9tVOf78L/r0qnelM5usXHDLYmzDGmFFvKJ78+Nl+LLMY19Mx/v0f6UtOclNL8zp9jsftznpjjOmroQgq0uNEkb8AzwMHikiliFwB3AScISJrgTP85yH1qyfXD/UmjTFmxEvrzY/d6PGUX1Uv6WbSaYOQlpTdu2wLV51iHU0aY0xfpKWkIiINIlKf5NWAu2dlRHj/UXvv09y0p5nTfvgESzd2be1sjDGmO2kJKqpapKrFSV5FqjoUpaG0WLhf5xv31+9q4tv/fiNDqTHGmJFnKK6pjBhtsfg+4+xyvTHGpM6CSoL3HtljN2XGGGN6YUElQWHOvjV1K7bUZiAlxhgzMllQ6eKCw/dtV/DG9voMpMQYY0YeCypdfP7MA/cZd84tT2cgJcYYM/JYUOlienl+0vH/Xrl9iFNijDEjjwWVFD36xs5MJ8EYY4Y9CypJ3HDewfuMCwR67G3GGGMMFlSSqijK2Wfc/csraY3EMpAaY4wZOSyoJHH+guQ9y9S3RIY4JcYYM7JYUElCJHlV16LvPEprJMb2upZO46saWtnVEB6KpBljzLA2YvrlGmp/v+oELvzFs/uMP+irDwHw8w8dwXGzx3HC9x6jNeK6d9l403lDmkZjjBlurKTSjcOnl3LeoZO7nf7yplr+9srWjoAymBrDUbueY4wZEayk0oOfXXIE/34t+f0ptz/79pClY/7XljB7fAGPff7kIdumMcb0h5VUehAICKceNCHTyQBgw+6mTCfBGGN6ZUGlF7/9n4Upzzvzun/zu2fe5tan1rNk9Q7C0Rjrqhr7tD1V5dv/fp03d1h/Y8aYkceqv3oR7ONNj9/81+sdwxOKcqhqCLP8htMZV7jvvS/J7Glq47dPv83fXtnGshtO79O2B+IvL21mTkUhi2aVD9k2jTGjj5VUUnDXFYv6tVyVb2Z81Lf+yz1LN/PbpzZQ3dTGeT99mrte2MSFv3iWd3zvMSIJDweLx91jwXY37ttEua8X62ub21i8YltK817/19f4wG+e79P6R7qZ1/2bW/67NtPJ6FDXHCEWt8fCmZFNVEfHj3jhwoW6bNmyQVv/uqoGTv/RU4O2fnCPM162qabj8xfOOpDCnCy+tng1AFNKcikryGb1tnq+//7D+OIDK7nvk8fRFosTiyvZwQDHzB7Xsfylt73IM+t28/QXTyEaVwpzsmhpixFX5eQfPAHAOfMnMbE4lzue2wi4ZtFfuG8F7z1yGsfN2buurjbubqK6uY0jZ5TtM62qvpUNu5s4dvY49jSGyc/Ooi0WZ2VlLe+cW5GGbyo9Zl73b2B4NAVvDEeZ/7UlXH7CLP7v3fMynRwzhojIclVNvZ6/F1b9laL9JxQN+jYSAwrA95es6fR5W10r2+paAfjC/SsBeP+vO5cuvve+Q2lojXLzQ2s6Ho9885I1/LObEst/Vu3o9LmhNcJ9yyu5b3klT3/xFJraolz++6Xc88njaGqLcsezG/nMGQd0BKW3vnUOH7vjJT57xgEcPr2MtmicRd95FIA13zqbo771307rT6wK/NL9K3ltax0PXvNOAJ5YU8WU0jwOmJj8u/7ry5W8vq2eL597MA+t3sFpB08gJyuYdN50qWpopTg3RG5ocLfT2BoF4F8rt1lQMSOaBZU+WHLtiZz1k8EtrQzUlx54bZ9x3QWUZA698eGO4Xfe/HjH8Enff5z2mpm7l27pGH/ADf8B4Nl1zxMMSKfqm/uXV+6z/qO+9V+mlOTy+48t4p5lbj0tbTGO/vZ/aQy7A+tPLzmCc+ZPIhZXckNBttW20BaN89l7VwCQlx3kZ4+tY3p5Hk9/8dSU89adh1btoDQ/xLEJpbxILE5TOMqib7sAmViaeXxNFRWFOcybXIxI9z0w9EX7KhKrQoez1kiM6x5YyZfPPZgJxbmZTo4ZRqz6q4/2NIY55QdPUO/PLM3g+ufV7+DdP3+m2+mzxhfwtm9u/djnTmJqWR6hQKCjV+mHVu3gU39cDsDjnz+Z+pYItS0RTjqgoqP6q93Gm87jiTVV3PHcRopzQ52uRz31hVNoi8XZf0Jhp+U+dsJMvnT2QQREyM7qfIlyZ30rWQFh/a4m8rODTCzO5fE3qzhyvzLuW7aFT540h3A0xuSSPJ5bt5sP3fZiRzq6s622he/+502OnzOOSxbNSOUr7OT3z77NolnlHDKlpGPcpj1NTCvL77VRytqdDTS3xVgwvZQHllfyuftW8J4jpvLjiw/vczpM+uyoa2VSSf8De7qrvyyo9FNdS4QFX3+49xlNRtzywcMpyQvx0d8vTXmZSxbN4C8vbe5xnj99/Bg+7A/+iWZXFPDY506mqr6V7KwAG3Y38d5fPpfSdjfedB7fe+hNfvXEegAOmVLMvz/tqgT/9koln7lnBbd+5CjqW6N8/r4VnZZbv6uR/crzUaCypoUfLFnD1y84hPG+ivHulzbzyuZavnzuwbxV1cBFvrp0zbfOJhZXvvPgG/zxhc38fyfP4YtnHwTAQ6u2kxsKcujUEtZWNXLYtBJe3lTLpb9z+f7USXM4aFIR197zKucvmMJPLzmiI02vVdZRkhdixrjkD7sDWLGllgMmFpGXnXqV4vJNNbzvV8/xwvWnDegAOhCLV2zjxLnjKc3Pzsj2k3l49Q6uvGs5d16+iJMO6N/1Sgsq3RjqoNLukdd38ok/DP12jWl3+sET+O8bVZ3G3fz+w/iiv+6WioX7lXH+4VO4cfFqUmmA9vkzD+AHD78FuAYl9a0R/vTC5o4qzIeufSeqkJ8dZHxhDtG4Utcc4am1u7jh76s46YAKrjplfxbNKmf5phrmTiykqr6Vi3/zAicdUMFfX9nKkmtPZNb4Ah5+fQc/f2wdb+5oAODt7567T5Xj7sYwuxrCLN9UwzGzypk7sYi2aJwDbvgPN733UA6dVsL9yys5Z/5kFs0qJxyN8YMla/jt029z7yeP44HllVx96v6dnvy6amsdi1ds45rT5rKnsY0Tv/84Jx5QwR8u79watLktyt9f2cYli6YjIuxpdOm48q7lHDixiCP3K+P9R03lqP3KeezNnXxt8WqWXHsiZ//kafY0hvnzJ45lwfTSTutsaI3QEokxoajnAPr9JW/yi8fX89kzDuDTp83tfcclYUGlG5kKKgCxuDLnyw9mZNvGjGRXnTKHXzy+vl/Llhdk88sPH8myjdUdAa7dDy5awJ3PbeS1rXWEgkIktvc4d95hk7t9PPgtHzycUw+awMOrd/K5hFJhu/3G5fN/75rHL59YT0leiAlFOZ2uMd76kaO48q7lSdf98w8dwdV/fgWAotwsGhKq0Dd851yicWXVtjqml+Vz9LddA5cH/vc4fvjwWzy3fg9vfesc7l22hePnjGN7nesZ/eePr2NdVSP52UFe/8bZKX5znVlQ6UYmgwq4O+G3VLcwY1w+dc0RFnzDqsaMMUOnv03jrUnxMCUiHfXIJfkhNt50HqrKH1/czFf/virDqTPGmKFhd9QPIhHhI8fux11XLOL4OePYeNN53Pep4wA4/eCJvHbjmZwxbyJHz9z3BkJjjBmJrPprmFpX1cjr2+s5f8EUmtuivLK5ll8/uZ7bLltIS1uM17fV86HbXmTV18/irB8/xVH7lVFekN1xZ/yvLz2SXz25gR99YAENrVEeWrWDXz+5ngMnFnHqwRM6WhoBTC3NY2d9K9GEK7SHTClm9TbXqeXE4hx21tuTLY0ZzoZL9dewDioicjZwCxAEblPVm7qbd7QFlf7aVttCYW4WxbmhXufdUt1MJBZndkUhABt2NaLAHP85GVUlElOyswLct2wLi2aV89eXt1JRlENeKMjNS95kZ32Yz595AO+cW8HyTTX8+aXNfOTY/bjs+Jls3N1EYW4W339oDfcs28LGm87jhQ17GF+YQ1VDKx+9fSltsThLv3I6F9/6PBt2uXtQ1nzrbA68wT118/ITZiV9ns2nT92fnz62jvs/dRxba1v47dMbWLV1396ei3KyCEfjHT0OGDMaWFDphYgEgbeAM4BKYClwiaq+nmx+CyrDQyyuPLV2FycfUJGWO83jcUXpvrfolrYYOVl7b3bsq6ZwlKygUNcSobUtzrSyPAAawlFQ10onEBDC0Rj/eGUbFy2c1ilf0VicbbWtTC3LIxKLE47Eyc8J8vTaXRw4qZippW59NU1trN5Wz9SyPLbXtjC7opA9TWFeq6zj3MMm09Aa5ZXNNZx0QAX1rVHe2tHAAZOKGFeQTV1LhPzsIFf9+RVysgJ8+8L57KwP87E7lnbqePT560/l5U21HDq1hLxsl4bsrACRWBxBKMzJ4pCpxSzfVMOX7l9JU1uMr5x7MN9fsrdLn++851Cml+cRENnnfpz3HDGVv72ytdO4/ScUsq6qkatP2Z+fP75un+/3Jxcfzq+fXN/RHLjdB4+ezt1Lt3DkjFJe3lzbr31n9irOzWLljWf1a9mxFFSOA25U1bP85+sBVPW7yea3oGKMSUVdS4Qt1c1ML8unJD95iT4WVwIJXfC0RmId/b/F4oqqEo7GCQZkn37h4nGlsS3aqbYgHI2RHQzQ3BajKRylriVCNK4cNKnIrz9OQziyz30pbdE4dS0Rmvz9P1NK8zp6bmiLxmluiw74Zsyx1PprKrAl4XMlcEziDCJyJXAlwIwZfe+ywhgz9pTkhSiZWtLjPF1LxomBw00TsoLJ2zkFArJP9XN7x6cFOVkU5GTt019aXnYwaQ8D2VkBKopyqCja93lM2VkBsrOGz9397YZz669k9RmdilWqequqLlTVhRUVw6dLdWOMGauGc1CpBKYnfJ4GpN7drjHGmCE3nK+pZOEu1J8GbMVdqP+Qqq7uZv5dwKYBbHI8sHsAy49klvexayznfyznHfbmfz9VTVtVz7C9pqKqURG5GliCa1J8e3cBxc8/oC9FRJal82LVSGJ5H5t5h7Gd/7Gcdxi8/A/boAKgqg8C1lOjMcaMEMP5mooxxpgRxoLKXrdmOgEZZHkfu8Zy/sdy3mGQ8j9sL9SbzBORG4H9VfXSQVr/auAqVX1C3F1mtwMXAmuBz+G65jkwzducAbwOlKhqLJ3rHitEZCPwcVX9b5Jp72QQ9psZOaykMsaJyIdEZJmINIrIdhH5j4i8Yyi2raqHqOoT/uM7cF3yTFPVRar6dDoOTCKyUUROT9jmZlUttIAyOFLdbyJyo4j8cSjSZIaWBZUxTEQ+C/wE+A4wEZgB/BK4IAPJ2Q/YqKpNGdj2iOeb4I8ZYy2/I4qqjukXcDawBlgHXJfp9KQxXxuB14BXgWV+XDnwCK566XGgEbgI13vBT/13sBI40s9/I/Ccn38t8BKwA6gDngIOSdjeubhqpQbcfUWf9+PHA/8CaoFq4GkgkJDG04ErgFYg5tP0deBkoDJh/dOBvwK7gD3Az/34OcBjfvm4306pn3YPrheGOBAFvgbM9ON+5vP7OvCkT9s6XBVce37/BtwL/AFo8tvY4r8rSfKd3+Kn1wPLgXcmTAsCXwbW++9oOTDdTzvE75dqYCfwZT/+DuBbCevo+p1sBL7kl1NgFXCd30YYiABv+9/AuX6ZTwBV/jsJA1cBXwAeoPN/4TngJ36ZWcCL/ju5x2/387jfSp0fl9tNGr+E+z00+HWf5rfT5tPXCKzw804BFifsi08krOdG4H7gj/77vQFoBg7F/Zbf8PluBEJ0/q0/ApT59ST9rftplyXs+8sSxh+F+y+t627fZ/B/Pj0h/6uBaxK+r61+33fsfz/tep+XNcBZvR0Lk+z/7B7TlOkvJcM7JOh/iLOBbGAFMC/T6UpT3jYC47uMu7n9x4I7eMZxzcrPBf7j/3DHAi/6eb7nDwblQBnugD4dyMGVcF5NWPd2/EHUz9semL4L/Nr/0UPAO9l7LW8jcLof/ijwTML6TsYfnPx+WgH8GCgAcoF3+Gn746rNTgNOxR38f5KQ3xpc4LrO52cm7gD8kM/vy7gDeS5wIi6wXejzUIMLJOfiAuqdwAv+uzonyXd+KTDOf6efwwXg9oPtF3AHpgP9dhf4eYv8d/c5n4Yi4Bi/zB30HlReBd4HHIcLKhfhDs5fB+7y38dkP/9FPq9v+X14Mu6G4al+vrdx/4U83AH//X65e4EP+uFf44L6S3475bgD2qeS7LcDcUF2iv88E5jjh28E/tjl+3sSV1LOBQ7H/d5OS5g/4vdNwKfxQVzQav+t/dzvs3l0/q1fB3zPD3f3Wy8HNrD3t76BvYHoJf/9Snf7PoP/88kJ+S/y+3ae/74+n2T+ebj/Ug4uWKzH/b+6PRYm2f//21Oaxnr11yJgnapuUNU24G4yU/UzVC7AHRjBnSnHVTXqx/9BnReAUhGZjCsFbFfValWtwZUU3qGqYdyPdoGItPfMFwHmiUixqtao6ssJ4yfj7tqNqKtz72vrkEW4A9gXVLVJVVtV9RkAVV2nqo+o6qO4A8Fu4KSE/Db64TtxB6R2d+G6/lmAO9suwx1cVwIX+vyux52dvQIU44LSAlzJJXFd+LT8UVX3qGpUVX+I++O2X1/4OHCDqq7x3/MKVd0DvAvYoao/9PlqUNUXu667Bz9V1QdwgQlVvU9Vt+EC5wqf/kUJaXgJ+L2qhtVdz3oTV+25EmhW1Q24AF0NzPUNKE7FlRLav8d8v91tqloN/BMXBLqK+e9gnoiEVHWjqq5PMh8iMh13Xe1L/nt4FbgN+EjCbM+r6t9VNa6qLT4t56vqy/5RGe/3+ZhK59964r7v7rd+FvBIwm/9EeBsP61YVZ/3v9uk+z5TVHV7+39NVRtwAX5qD4tcANzt9//buFLJIro5Fnaz/3vM/1gPKsl6Qu5ph4wkCjwsIst9b84AE1V1ux9eDwR93XR330Mxroqh/fk2BwM/E5F63FkyuOotcGfL5wKbRORJ/+gCgO/jfrgPi8gGEbmuH3mZDmzyAbATEZkgIneLyFZcSWBaQpom4g5s+HxP6JLHKbiD5xaf36nAZvb+Bupx1TRT/fzNuLPo7ST5nYjI50TkDRGpE5FaoCQhLdNx33myvCU90KYocb8hIv8jIq/izs5vxgXBz4tImd+WkHxfv4A7SwdX4nrSjx8H1CZ895W4s9odCetoBvZ5spuqrgOuxZ2AVPn9NKWbfEwBqv2BsV17KSppXoF/4ALWbFxptYm9VTUdv/Uu+76733pP4yuTjB92RGQmcAQu/wBXi8hKEbnd73/oe/6T7f8e8z/Wg0qvPSGPYCeo6pHAOcBVInJil+nP+/cLSe17+BAuqPwGd7Cc6ccLgKouVdULcH/ev+OKzPgz78+p6mzg3cBnReS0PuZlCzCjm4uz3/VpPQxXv17ZJT897c9tuANp0M8nQCmuLjpRr9+Pb0r7JeADuGqTUlwJqH3ZLbiSX1fdjQd3kMxP+Dypl3SEgN8CV+MOriFcaaUe+KHfVrI+3xXXt165iMzHlZ6eY+930m+q+mdVfQeuIYbiSntd0w1+X4hIUcK4GXTeF117KW/F/c4+DHwMF/CvVdV9H/e5V3f7sq/jhxURKcRdF2vP/69wv6vDcSdBP2yfNcniac3/WA8qo7YnZF8FgqpW4S44LwJ2+uI8uIPVLuAXuGsAc0QkJCLn4M52tuEORu0HtSLcD2y9H/ed9m2JSLaIfFhESlQ14peL+WnvEpH9fTG6fXxfm/O+hPtj3CQiBSKSKyInJKSrEXeBfiJ7Swbgrh9UA7N9vqsSpk1T1S24g+cC3HWCAHA08Cc/T3tJrRL322g3lX1/J0W4xgC7gCwR+T+/fLvbgG+KyFxxDhORcbhGDJNE5FoRyRGRIhFpf27Qq8C5IlIuIpNwZ/09CeD+8Lt8Xv8HmI9rVLHIp+FQ4Bifhv1x16S24a6nVAF/xn3feX78blwVUXtAn0aK+09EDhSRU0UkB3dtqiVh2Z3ATBEJACTsi+/6/XsYrgHHn5KsOtEfcNfj3ofrH/Cv7etv/6132ffd/ed7Gj8tyfhhQ0RCuIDyp/b8q+pOVY2pahx3otFLwZ4IAAAgAElEQVReBdrX/Cfb/z3mf6wHlaW4euNZIpINfBDX+mRE8wfeovZh4EzcRdzFuBYu+Pc7gM/iLs79DncmewOujn87LoBM8UXnxbgDzS24FlMvdNnsR4CNvmrsU7gqFIC5wH9xB/7ngV/q3ntTUqLunpJ34w6Am3F/gIv95K8DR+JKBb/HBa52i32+b8Bdb0n8M3zEB7qb/eeXfbqbgWU+z+3XlLbjGiwckZDXf3RJ5hLcRdy3cNU27S3F2v0Id1b9sE/j74A8X91zhs/fDtw1kFP8MnfhShob/XL39PQ94Vpz/RD3PVfhAsizPt2rVPU+3P77hM/Pv30eX8L9FwJ+mT/j/wv+OsLjuOsV4H43zb2ko10OcBPuwLQDV4r9sp92n3/fIyLt198uwZWAt+FOhL6mqo/0so3ngArcb/arCeO7/tb/kTD+f3xQPRao8/t3CXCmiJT5fX8msKR934vIsf738j/su+8zxqfpd8AbqvqjhPGTE2Z7D+5/AC7/H/QnMLNw/8/2/b/PsbCb/d9z/nu6ij8WXrjrAG/hDqBfyXR60pSn2biD0QpcM8Ov+PHjgEdxB65HgXI/XnAllvW46xILE9Z1Oe6ayDrgY5nOWw95/guuNBPBBZ0r0plfYKH/Y67HtTIaTs1Kk+X9Lp+3lf5AMjlh/q/4fKwhoSWTP2DEcQH4KwnjZ/sDzzpcMMjJdJ4T0vYOXOlsCwnNZ8fQvm/P/8ou+e/P/k96LOzr/u+1mxYRKVfXwsMYM0r5aqgf4Vo6XZ7p9KRKRI7GtdSarp0v8psMSaX660URuU9EzvVFrZT4FgdVIrKqm+kiIj8VkXW+hcKRCdMuE5G1/nVZsuWNMenhq0jrcdVwX8twclImInfiqlavtYAyfKRSUhHczWOX4y723APcoapv9bLcibh69D+o6vwk088F/h+uyHUMcIuqHiMi5cAyXJFTcfdTHKWu7bgxxphhrNeSijqPqOoluJunLgNeks73IiRb7ilcy5vu9OkmpD7kyRhjTIb02imbb/Z4Ka7Fy05c6WIxrv3zfbj28P3R15twkqXtSuBKgIKCgqMOOuigfibFGGPGpuXLl+/WIX5G/fO4lgQXqmrinaXLROTXA9j2gG+2UdVb8Q+aWbhwoS5btmwAyTHGmLFHRDalc32pBJUDtZsLL6r6vWTjU9TTTTgndxn/xAC2Y4wxZoikElQeFpGLVLUWwN8YdLeqnjXAbS/G9U1zN+5CfZ2qbheRJcB3EvqqORPXVbMxxoxZrZEYNc1ttEbitEZihKPuvTUSoyAni6Nnlve+kiGQSlCpaA8oAKpaIyITeloAQET+gitxjBeRSlxTxZBfx69x3Vafi7uhphnXdw+qWi0i38Td4QnwDbtPxhiTqK4lAkBJXqjXeVWVdVWNvLGjgexggNxQgNxQkJws916QnUV5YTYF2UFSvWtCVWlui9EUjtIYjtIUjvn3KE1tURpa/XA4SmM4RmvUHfzDPiC0Rt1wcV6IicU5TCjKZVJJbsdwfUuE9bubWF/VyPpdjWzY1cTW2pZu03P49FL+ftUJ3U4fSqkElZiIzFDVzQAi0t4xXI98a7GepivuAUHJpt2Oe96HMWYMa2iNsGFXE2t2NvDWjgbeqmrkrR0N7KhvRQQOm1bKiXPH8865FRwxo5RQ0DVojcWVZRureeT1nTzyxk427em9Z5mcrADjCrIZV5jDuMJsinNDtERiCcHBBY/2wBFPoVtJESjIziI3FCQ3FOgIZDlZAXKyguysb2VlZS27G9uSLp+fHWR2RQFH7VfGBxZOp6IopyMo5oYC5GYFyQkFUwquQyWV+1TOxl0Mf9KPOhG4UlWXDHLa+sQu1JuRTFWpagizflcj63c1UZgT5NSDJvZ6sKhtbuOhVTtYs7OB4twQpfkhSvL2vhfkZBGNKbG4Eo0r0VicWFwREaaV5TG5JJesYPd3FtQ0tbFhdyObq5uJxpIfK4pysyjOC1Gal01JfojSvBD52UFUoamty1l8OEo4FicWU6LxuE+TEonFqW5qY2ttC9tqW6isaWFrbQsNrXufdpCTFWD/CYUcOLGIAyYV0dIW45l1u3llcw1xhcKcLI6dPY7ivCwef7OKmuYIoaBw3JzxnDFvIkfPLCMWV1ojccLRvaWGprYY1U1h9jS2sbuxjT1+uL41Ql4oSGFOFgU5Wf49SEFOFkV+3N7xblqh/9w+Li8UJBDovfQTicXZ1RBmZ30rO+vDFOVmMbuigEnFuSmXnvpLRJar6sK0ra+3oOI3Oh73lDTBPShnd7oSkC4WVMxgUVXe2tnI8+t389z6Pby6pZb5U0u46KhpnHrwBHKygn1e59baFpas2sFrW+s6qjcaw50fFxMKCsfPGc858ydx5iGTKC/IBtzZ+yOv7+RfK7fz1Fu7iMaVvFCQlkhfO3+GrIALLjPGFTBzXD4VhTlU1rS4NO1uorop+Rl0KuuNpnIq30VRbhZTS/Pcq8y97zeugAMnFTGjPJ9gkgN0XUuE59fv4em1u3hq7S7qW6KccmAFZ8ybxIkHjKcod/icxQ9HmQoqZbjeLHPbx/mbG4cNCyqjU0NrhNrmCNPK8gZ0xra7McxrlXXUt0Y6zpobwzEaW6O0RGJkB8VVSyTUtQO8srmGFzbs6aiemF6ex4JppSzdWM3O+jCl+SEuPHwq7z9qGodMKe4xjRt3N/GfVTt4aNV2VlTWATClJJc5EwqZU1HI7IqCjvcdda08tGoHD67azpbqFoIB4ZhZ5RTlZvH4ml20ReNMLc3jXYdN5l2HTWH+1GJicaWhNUptS4Q6/2oORwkGhKygEAwECAWEoD/gb6luZlN1M5v3NLOpuolNe5ppaI0yvjCb2eMLmTOhoON9v3EFZCcp0ahCYzhKbUsb9S1uX7VvOxQM7HMWn5+dRU4oQCgQIBgQQkGXnqxAgNKCEMUWAIbckAcVEfk4cA2uae+ruBLL86p6aroSkQ4WVEaHWFxZWVnLU2/t5um1u3hlSy2xuFKaH+LQqSUcNq2Ew6aVsmBaKROLc7o9iNc1R3jh7T08v9691uxM3jVUQXaQvOwgbdE4Yf9KNLE4h+PnjOe4OeM4bvY4ppfnd6Tz6bW7uH95JQ+/vpO2aJyDJhWx/4RCsgJCVjDg3wVBWLaphje2u175F0wr4ez5kzln/iRmji/o8ftQVVZvq+ehVTv4z6rtNIVjnD1/Eu9eMJkjppelVLWSKlUlHI13BNR9xKLQ1gDhhFdbI4QKIK8U8sogtxRCuZ2Xi4b9/PXuHYGsXMjK8e/Z7l0CLkpp3L3ww7EoxMJuPbG2ve8ah0AQAlmdXwDxGMSjCa+YWybSApHmzu+xNgjl+Ve+f+W59MWjbnqs/b0NYhGfxphbb3t6Ne7yEAxBIOTfg24Y9i6TmMdIi381ufe2ZpcuCbjvJJQLWXn+Pcl3pH77hZNgwcX0RyaCymu4Bxe9oKqHi8hBwNdVtX85GCSjMahEY3Gqm9vY0+hfTWEaWqPuIl8oSG7CRb/ygmz2n1A4oLP5+tYIj79ZxbiCHOZNKe6obulNPO6uB2ytdfXgW2tcvfjeQFDKpJLcpMvWt0Z4c3sDq7fVsXRjNc+s3U19axQROHRqCe+cO55JJXms3lrHiso63trZQMxXqxTlZJGXHdx70TIUJDcrSGM4yhs76lGF3FCAo2eWc/yc8SycWUZ5QXbH2XN+kvrueFxpi7m69mhcGVeQ7b7TeMwdFFtq3Xsov+MgWhdW/rlyG4tf3cbuxjDRuLuGEYnFicXiSDzC/IoszjqgiFNmFzApL+4OHm1N7mDSPtzW6N+b3MEsGHIHyY6DVJZ7DwRB2g+mAfeu8b3Lhhv2DkcTWwz5vIq44fYDsgT9cNCNDzdAa517hev3DkdSfIxKVh7klkA84tYV618V2pgjQcgucAFNFaKtLtDEI70vO3UhfOLR/m02zUElldZfraraKiKISI6qvikiB6YrAaazVzbXcNszb/Psut3UNqfwY0owe3wBFx4xlQsPn8qMcfm9L+Ct2lrHH1/YxD9e3dapXn5ySS7zJhczb0oxB00qJhqPs6POXUjc2dDKzrpWdja0sqOulUiXi7jFuVk0tcU6AsCEohwOm1bCgqlFBOMR1myvZe2OWrbVNBEkToA4M4uUT84SFk2IMa+4jYLoamjaBbujMH4yzJpMOG8C61qLebUuj3V1QSJtrcTbmoi3taCRZjTSQm52nEuPKeXQ6WUcNLmUUCgEosAeFxRqqqGlBpqroaXajfNnrYFIC7mRZnKjre5g33Fwrev2+yvJKebSvFIuzS2FAn9wjzSDtkC0CYi5ZzHuwj0yqycSgOxCd4CPRd0BJRZxZ6SpCuW7g1N2gTvAi7iDFNDRcFPj/izbn223D2sccopcUMgtgeLJkFPshnOK3bT2V26xK6VEmtz32VLr3ltr3XBWTsL8ftls/yj7aKsrcSS+t3eoIYGEl+w968/KgWC2f8/xZ+1JSiQa37f00h5A2w/a7aWRUJ5bfzS8bwkm2tq51BHMdq+OYBzYG+Db06sxt7/iEV868sOJ+Qr4+SGhhFTgtpHspDAe8wGmdW9pSLqur+/X9QZLKiWVv+HuIbkWOBWoAUKqeu7gJy91I7mkEosrD6/ewW3PvM3yTTUU5WZx7vzJTC7NZVxhDuMTmjkW5WT5appYx01QrZE4m6ubWbxiKy9scLf0HLVfGRceMZVz57sLvF1LMK2RGP9auZ17nl9L89bVzA9t5dwJ1SzI20U0kMPOaBFbwnmsbcxhbWMOe+IFlNDEZKlmRlYN+4VqmSLVVOgesgKKZuUTyM4jmJNPKLeArOx8YtE2WhpriTbXQbiB7GgD+bT27cvJKXF/mpZBulVJAu6AmV3oqxvaDzi57r394Jpb6t7zSt3BMdLqD6RdXoGgP6jnuwNFdv7eg3yn9/bpXV5Zud0cWOL+oBlJCAZ+nMYA2buOwPA5wJjhLyMX6hM2fhJQAjykqsOqTJvJoBKJxXlzewOvbKkhEtOO1ivTcpoobXgLqXrTndUd9C7ILSYai1PfGqWuJcITa6q4/dm32VLdwvTyPK44fgYXV2wmr+rVvWe8He/NgELhRCieAkWT3XvxFMgfD/EIO2rqeXzVFp58o5Ide+rJIUKJNFIWaKJMmigPNFEqjZRqHXO0kpmBnQTx1xGC2TBuf1dd0bzHHSSTycqDkqlQPNVtOxD0Z3itnc/2gqG9Z6i5xZBTTDhYAKFcckLZCdU4/swtlA+FFVDgX/nj3FkpuDPJhh3QsB3qt7nh1jo3PZS3NyC019V3PRNXn8fcUsgvd1VX+eU+aI31p2qbsWxIg4p/GtzKZM9DGW6GLKio0rDxZdatW8PGnTVs2V3Ljuo6ArE2sokyVXZzoGzmwEAlFdK5yiRMNo/pQu6NnMDT8UOJ+trHhTNK+eyhLRzb9BiB1X91B07wB1pfXG8/swU3vZ9n7jEJ0hIspiWrhGDFAZTNWoBMPAQmzIPyOa7evmPmqNtO8x5XVZRb4oJJbmnys2ljzIgzpNdUVDUuIisS76gfs6reoPnle2l95V7Kw5UcARzRPi1Ax5NpNCuPltK5VBeexouhWaxlBivaJlPStpOTWx/jpPrHOEeeoyVUxtbp51FUNpGJm/4Jj77l6m7nngmHvh/mnuGqZLo7eEdaXXBpP3NvqXH1vO31zu11z1k57qzcv4LZhRSKUJhKnoNZUDjBvYwxJgWpXFN5DNf66yWgqX28qp4/uEnrm0EpqdRugdfuJbbyfoK7XiemwvN6CFunnsv8o05g/8njyMnJTbh4mO3O5nuq0462wbpHYMXd8NZDrqppvxPg0Itg3gWuSsYYY4ZIJlp/fT1dGxtR4nH0V8cj4XpW6oH8LXoZ0YPO5/KzjuUdE1I6z08uKxsOOs+9WmrdtYKiielLtzHGZFCvQUVVn+xtntFIW2qQcD3fjVzCxoM+zmfOOICDJhWndyN5peldnzHGZFgqjxNuYG+vxNm47uubVDXNR9jhpbFmJ0XAkfPncf0laSsZGmPMqJZKSaUo8bOIXAgsGrQUDRON1TsoAgrKrGrKGGNS1ecG+qr6d9xNkKNaU20VADkl1vLJGGNSlUr113sTPgaAhaTwkK6Rrq1+FwCFVlIxxpiUpdL6690Jw1FgI3BBKiv3D/i6BQgCt6nqTV2m/xg4xX/MByaoaqmfFgNe89M2D3UT5miDCyrF5VZSMcaYVKVyTeVj/VmxiASBXwBnAJXAUhFZrKqvJ6z7Mwnz/z8S7icEWlT18P5sOx3iTXto1hzKSq2FljHGpKrXayoicqeIlCZ8LhORVJ4fvwhYp6obfD9hd9NzCecS4C8prHdIBFp2U0MR+dmpFOaMMcZAahfqD1PV2vYPqlpD5xJFd6YCWxI+V/px+xCR/YBZwGMJo3NFZJmIvOBbnA2pULiG+kDJUG/WGGNGtFROwwMiUuaDCSJSnuJyyTqt6u4C/weB+1U7PTRihqpuE5HZwGMi8pqqru+0AZErgSsBZsyYkUKSUpfbVsOeLKv6MsaYvkilpPJD4DkR+aaIfAN4Drg5heUqgekJn6cB27qZ94N0qfpS1W3+fQPwBElKR6p6q6ouVNWFFRUVKSQpdfnROsIhCyrGGNMXvQYVVf0D8D5gJ+7Zde9V1btSWPdSYK6IzBKRbFzgWNx1Jv8UyTLg+YRxZSKS44fHAycAr3dddjAVxeuI5FjnjsYY0xep3KdyLLBaVX/uPxeJyDGq+mJPy6lqVESuBpbgmhTfrqqrfWlnmaq2B5hLgLu1c3fJBwO/EZE4LvDdlNhqbNBFWsmnlXieBRVjjOmLVK6N/Ao4MuFzU5JxSanqg8CDXcb9X5fPNyZZ7jng0BTSNiha66vIBaRgfKaSYIwxI1Iq11QksRShqnFSC0YjVkP1DgCChem9TmOMMaNdKkFlg4h8WkRC/nUNsGGwE5ZJjdU7AcgptqBijDF9kUpQ+RRwPLAV16LrGHwz3tEqXOc6k8yzfr+MMaZPUummpQrXcmvMiDRYZ5LGGNMfqbT+ygWuAA4BctvHq+rlg5iujIo17iGuQlm5VX8ZY0xfpFL9dRcwCTgLeBJ3E2PDYCYq45p3U0MhJQW5vc9rjDGmQypBZX9V/SruEcJ3AueRwea+QyHYWk29FBMIJOtpxhhjTHdSCSoR/14rIvOBEmDmoKVoGMgJ19AQtM4kjTGmr1K53+RWESkDbsB1s1IIfHVQU5VhudFaarKSdqhsjDGmB6m0/rrNDz4FzB7c5AwPhbE62vJHdQ2fMcYMilSqv8aWeJxirSeaW5bplBhjzIhjQaWLeEsdWcQhf1ymk2KMMSOOBZUu2vv9Cli/X8YY02cpdQwpIsfjWnx1zO+fszLqNFTvoATILrIeio0xpq9SuaP+LmAO8CrQ/rhfBUZlUGmudZ1J5pZYFy3GGNNXqZRUFgLzujxEa9QK17t+v/JLJ2Q4JcYYM/Kkck1lFa6bljEh2rAbgOLxYybLxhiTNqkElfHA6yKyREQWt79SWbmInC0ia0RknYhcl2T6R0Vkl4i86l8fT5h2mYis9a/LUs/SADXvpkWzKSspHbJNGmPMaJFK9deN/VmxiASBXwBn4J7DslREFid51vw9qnp1l2XLga/hqt4UWO6XrelPWvqU7pZqailicig42JsyxphRp9eSiqo+CbwJFPnXG35cbxYB61R1g6q2AXcDF6SYrrOAR1S12geSR4CzU1x2QEKt1dQHrN8vY4zpj16Dioh8AHgJuAj4APCiiLw/hXVPBbYkfK7047p6n4isFJH7RWR6H5dNu9xILc1ZVvVljDH9kUr111eAo/0TIBGRCuC/wP29LJes3/iuLcj+CfxFVcMi8ingTuDUFJdFRK7EP9p4xowZvSQnNQXRGqryrDNJY4zpj1Qu1AfaA4q3J8XlKoHpCZ+nAdsSZ1DVPaoa9h9/CxyV6rJ++VtVdaGqLqyoSM8d8EXxeiI51u+XMcb0RyrB4SHf8uujIvJR4N/AgykstxSYKyKzRCQb95z7Tq3GRGRywsfzgTf88BLgTBEp893un+nHDa5omAJaiOdZv1/GGNMfqXR9/wUReR9wAq5a6lZV/VsKy0VF5GpcMAgCt6vqahH5BrBMVRcDnxaR84EoUA181C9bLSLfxAUmgG+oanXfs9c3LXU7yQOkwIKKMcb0R0p9f6nqA8ADfV25qj5Il1KNqv5fwvD1wPXdLHs7cHtftzkQdbtdUMmyziSNMaZfug0qIvKMqr5DRBrofJFcAFXV4kFP3RBr7/cru8S6aDHGmP7oNqio6jv8e9HQJSezWupcewTr98sYY/onlftU7kpl3GgQaXCdSRaWW79fxhjTH6m0/jok8YOIZLG36e+oEm/cTVyF0nIrqRhjTH90G1RE5Hp/PeUwEan3rwZgJ/CPIUvhUGreQy0FFOfnZDolxhgzInUbVFT1u/56yvdVtdi/ilR1nG+1NepktVZTL8WIJLuh3xhjTG9SuU/len8D4lwgN2H8U4OZsEzIDtfQFLTOJI0xpr9SeZzwx4FrcF2lvAocCzyP66NrVMmL1lIVmpLpZBhjzIiVyoX6a4CjgU2qegpwBLBrUFOVIYWxOtqyrd8vY4zpr1SCSquqtgKISI6qvgkcOLjJygBVirWeWF55plNijDEjVirdtFSKSCnwd+AREakhSY/BI120uZYQMdQ6kzTGmH5L5UL9e/zgjSLyOFACPDSoqcqAuj07GAcEC8dnOinGGDNipXJH/bEiUgQdjxZ+HHddZVRprHb9foWKrTNJY4zpr1SuqfwKaEz43OTHjSotta7fr7xiu5veGGP6K5WgIqra0UuxqsZJscv8kSRc74JKQfnEDKfEGGNGrlSCygYR+bSIhPzrGmDDYCdsqEUbXSvpYutM0hhj+i2VoPIp4HhgK+7Z8ccAVw5mojJBm/bQqiFKS0sznRRjjBmxUmn9VYV7vvyoFmzZQ60UMykrmOmkGGPMiNXTkx+/qKo3i8jP6PzkRwBU9dO9rVxEzgZuwT2j/jZVvanL9M8CH8c9o34XcLmqbvLTYsBrftbNqnp+alnqn6xwDQ2BYqzyyxhj+q+nksrr/n1Zf1YsIkHgF8AZuGqzpSKyWFVfT5jtFWChqjaLyP8CNwMX+2ktqnp4f7bdH3mRGhqDVvVljDED0VNQuRj4F1Cqqrf0Y92LgHWqugFARO4GLmBvsEJVH0+Y/wXg0n5sJy3yo3XsybPOJI0xZiB6ulB/lIjsB1wuImUiUp74SmHdU4EtCZ8r/bjuXAH8J+FzrogsE5EXROTCZAuIyJV+nmW7dg2sj8vieB2RXOtM0hhjBqKnksqvcd2xzAaWA4lPrlI/vifJnnS1z7UZABG5FFgInJQweoaqbhOR2cBjIvKaqq7vtDLVW4FbARYuXJh03anQaJhCmolbv1/GGDMgPT358aeqejBwu6rOVtVZCa/eAgq4ksn0hM/TSNIRpYicDnwFOF9Vwwnb3+bfNwBPMIhdwzT5u+mlwPr9MsaYgejpGfXFfvArXau+Uqz+WgrMFZFZIpKNa5a8uMs2jgB+gwsoVQnjy0Qkxw+PB04g4VpMutXv2QFAlnUmaYwxA9JT9defgXfhqr6UPlZ/qWpURK4GluCaFN+uqqtF5BvAMlVdDHwfKATu88+Fb286fDDwGxGJ4wLfTV1ajaVVY42LZ7nW75cxxgxIt0FFVd/l32f1d+Wq+iDwYJdx/5cwfHo3yz0HHNrf7fZVuM71UJxXZv1+GWPMQKTS9f0JIlLghy8VkR+JyIzBT9rQiTTsBqDIOpM0xpgBSbXr+2YRWQB8EdgE3DWoqRpisUYXVErHWVAxxpiBSCWoRH3X9xcAt/gbIYsGN1lDK9C8h1otID83J9NJMcaYES2VoNIgItfj7nb/t+9+JTS4yRpagdZq6gIl+MYCxhhj+imVoHIxEAauUNUduLvivz+oqRpiuW01NAVKMp0MY4wZ8VLp+n4H8KOEz5uBPwxmooZabqSW6pD1T2yMMQOVSuuvY0VkqYg0ikibiMREpG4oEjdUiuK1tOVYv1/GGDNQqVR//Ry4BFgL5OGef/KLwUzUkFKlJF5PLDeVTgKMMcb0pNfqLwBVXSciQVWNAb8XkecGOV1DJtxcS47EIN+6aDHGmIFKJag0+767XhWRm4HtQMHgJmvo1Da28mzsHRSPm5fppBhjzIiXSlD5CK7vrquBz+B6Hn7fYCZqKE2YMImzbvgHAWtObIwxA5ZK669NfrAF+PrgJmfoiQgFOSnVAhpjjOlFt0dTEXmNbh6qBaCqhw1KiowxxoxYPZ2iv2vIUmGMMWZUENetV5IJIvsDE1X12S7j3wls6/po30wTkV24zi77azywO03JGSnGWp7HWn7B8jxWDCTP+6lqRboS0lNJ5SfAl5OMb/HT3p2uRKTDQL8UEVmmqgvTlZ6RYKzleazlFyzPY8VwynNPNz/OVNWVXUeq6jJg5qClyBhjzIjVU1DJ7WFaXroTYowxZuTrKagsFZFPdB0pIlfgnls/2tya6QRkwFjL81jLL1iex4phk+eeLtRPBP4GtLE3iCwEsoH3+N6LjTHGmA7dBpWOGUROAeb7j6tV9bFBT5UxxpgRqdegYowxxqQqla7vRzUROVtE1ojIOhG5LtPpGQwicruIVInIqoRx5SLyiIis9e+j6oEyIjJdRB4XkTdEZLWIXOPHj9p8i0iuiLwkIit8nr/ux88SkRd9nu/xHcSOGiISFJFXRORf/vOozi+AiGwUkddE5FURWebHDYvf9pgOKiISxD0b5hxgHnCJiIzG7orvAM7uMu464FFVnQs86j+PJlHgc6p6MHAscJXft6M532HgVFVdABwOnC0ixwLfA37s81wDXJHBNA6Ga4A3Ej6P9vy2O0VVD0+4P2VY/LbHdFABFgHrVBNMu0cAAAQWSURBVHWDqrYBdwMXZDhNaaeqTwHVXUZfANzph+8ELhzSRA0yVd2uqi/74QbcQWcqozjf6jT6jyH/UuBU4H4/flTlWUSmAecBt/nPwijOby+GxW97rAeVqcCWhM+VftxYMFFVt4M7AAMTMpyeQSMiM4EjgBcZ5fn2VUGvAlXAI8B6oFZVo36W0fYb/wnwRSDuP49jdOe3nQIPi8hyEbnSjxsWv+2x3ud7soeoWMuFUURECoEHgGtVtV5G+XNz/NNZDxeRUtwtAQcnm21oUzU4RORdQJWqLheRk9tHJ5l1VOS3ixNUdZuITAAeEZE3M52gdmO9pFKJe+hYu2nAtgylZajtFJHJAP69KsPpSTsRCeECyp9U9a9+9KjPN4Dq/9/e/YRKXYVhHP8+3DIulUQaIohJ6CqQiHChLULChYSbChMDkVZubFNEbYLIhRsXYpsiF0EFbjRXotgfiqJcRKW4C3Hhn3QhEUSEPC3OOzaILgbOOPfOPB8YfueeGS7nwG94z5/5vcc3gK9p+0mPSBoMIKfpHt8EbJN0gbZ0vZk2c5nW/t5i+1Jd/6ANHjawQO7tWQ8qZ4B19WuRJcArwPEJt+leOQ7sqvIu4IsJtqW7Wlv/GDhv+8DQW1Pbb0mP1QwFSfPA87S9pK+Al+pjU9Nn22/bXmV7De27+6XtnUxpfwckPSjp4UEZ2AKcZYHc2zP/nIqkrbTRzRxw2Pa+CTepO0mfA8/R0mNfBd4FjgFHgNXAReBl27dv5i9akp4FvgV+4//19ndo+ypT2W9J62kbtHO0AeMR2+9JeoI2kn8U+Bl41fY/k2tpf7X89YbtF6a9v9W/o/XnfcBntvdJWsYCuLdnPqhEREQ/s778FRERHSWoRERENwkqERHRTYJKRER0k6ASERHdJKhEjEDSzcoMO3h1S9onac1wJumIxWjW07REjOpv209NuhERC1VmKhEd1PkW++s8k58kra36xyWdlvRrXVdX/QpJR+vsk18kbax/NSfpozoP5WQ9GR+xaCSoRIxm/rblr+1D7/1pewNwiJalgSp/Yns98ClwsOoPAt/U2SdPA+eqfh3wge0ngRvAi2PuT0RXeaI+YgSS/rL90B3qL9AOyPq9Ellesb1M0nVgpe1/q/6y7eWSrgGrhtOHVIr+U3XIEpLeAu63/f74exbRR2YqEf34LuW7feZOhnNU3ST7nrHIJKhE9LN96PpDlb+nZdAF2Al8V+XTwB64dbDW0nvVyIhxyigoYjTzdbLiwAnbg58VPyDpR9pgbUfV7QUOS3oTuAbsrvrXgQ8lvUabkewBLo+99RFjlj2ViA5qT+UZ29cn3ZaIScryV0REdJOZSkREdJOZSkREdJOgEhER3SSoRERENwkqERHRTYJKRER08x8FymEkX/W/1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = model\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print ('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "# print(y_val_pred == data['y_val'])\n",
    "\n",
    "y_test_pred = np.argmax(best_model.loss(data['X_test_0']), axis=1)\n",
    "# print(y_test_pred)\n",
    "# print(data['y_test_0'])\n",
    "print ('Test set accuracy: ', (y_test_pred == data['y_test_0']).mean())\n",
    "\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, label='train')\n",
    "plt.plot(solver.val_acc_history, label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "with h5py.File('Predicted_labels.h5', 'w') as H:\n",
    "    H.create_dataset('label', data=y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
